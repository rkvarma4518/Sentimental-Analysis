{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mkgh9SoBSOwH"
   },
   "source": [
    "# Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_UNRNEQQG9Sr"
   },
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6-o7BjJTzVx"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TLZbzW6wT2ZQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aK8lU0OUUlX0"
   },
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d2I6egUxUoaq",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 15)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('dataset.csv')\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
      "0  570306133677760513           neutral                        1.0000   \n",
      "1  570301130888122368          positive                        0.3486   \n",
      "2  570301083672813571           neutral                        0.6837   \n",
      "3  570301031407624196          negative                        1.0000   \n",
      "4  570300817074462722          negative                        1.0000   \n",
      "\n",
      "  negativereason  negativereason_confidence         airline  \\\n",
      "0            NaN                        NaN  Virgin America   \n",
      "1            NaN                     0.0000  Virgin America   \n",
      "2            NaN                        NaN  Virgin America   \n",
      "3     Bad Flight                     0.7033  Virgin America   \n",
      "4     Can't Tell                     1.0000  Virgin America   \n",
      "\n",
      "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
      "0                    NaN     cairdin                 NaN              0   \n",
      "1                    NaN    jnardino                 NaN              0   \n",
      "2                    NaN  yvonnalynn                 NaN              0   \n",
      "3                    NaN    jnardino                 NaN              0   \n",
      "4                    NaN    jnardino                 NaN              0   \n",
      "\n",
      "                                                text tweet_coord  \\\n",
      "0                @VirginAmerica What @dhepburn said.         NaN   \n",
      "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
      "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
      "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
      "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
      "\n",
      "               tweet_created tweet_location               user_timezone  \n",
      "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
      "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
      "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
      "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
      "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  \n"
     ]
    }
   ],
   "source": [
    "print(dataset.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfM237zDUwSx"
   },
   "source": [
    "Segregating Dataset into Input & Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(dataset['airline_sentiment'].isnull())\n",
    "print(dataset['text'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ReRaankPU1f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@VirginAmerica What @dhepburn said.'\n",
      " \"@VirginAmerica plus you've added commercials to the experience... tacky.\"\n",
      " \"@VirginAmerica I didn't today... Must mean I need to take another trip!\"\n",
      " ... '@AmericanAir Please bring American Airlines to #BlackBerry10'\n",
      " \"@AmericanAir you have my money, you change my flight, and don't answer your phones! Any other suggestions so I can make my commitment??\"\n",
      " '@AmericanAir we have 8 ppl so we need 2 know how many seats are on the next flight. Plz put us on standby for 4 people on the next flight?']\n",
      "['neutral' 'positive' 'neutral' ... 'neutral' 'negative' 'neutral']\n"
     ]
    }
   ],
   "source": [
    "features = dataset.iloc[:, 10].values\n",
    "labels = dataset.iloc[:, 1].values\n",
    "print(features)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQwlfdWsj2LT"
   },
   "source": [
    "Removing the Special Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "uhesmp0CU9xc"
   },
   "outputs": [],
   "source": [
    "features_list = []\n",
    "\n",
    "for sentence in range(0, len(features)):\n",
    "    # Remove all the special characters\n",
    "    feature = re.sub(r'\\W', ' ', str(features[sentence]))\n",
    "\n",
    "    # remove all single characters\n",
    "    feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', feature)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', feature) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    feature = re.sub(r'\\s+', ' ', feature, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    feature = re.sub(r'^b\\s+', '', feature)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    feature = feature.lower()\n",
    "\n",
    "    features_list.append(feature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meD0mcOVj5rK"
   },
   "source": [
    "from text to vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zzisF0taVA_b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\n",
    "new_features = vectorizer.fit_transform(features_list).toarray()\n",
    "print(new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2vFlF_fj-SK"
   },
   "source": [
    "Splitting Dataset into Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Q4fpb6RmVI0t"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(new_features, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9zzG3lDkC9L"
   },
   "source": [
    "Loading Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0TG77VbjVK7H"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, random_state=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "text_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-cE4paSkH69"
   },
   "source": [
    "Predicting the Test data with Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JjNeO6rQVMfr"
   },
   "outputs": [],
   "source": [
    "predictions = text_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzNioePqkMnH"
   },
   "source": [
    "Accuracy Score of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "o2TY6JdyVOXn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7599043715846995\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "26_SentimentalAnalysisNLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
